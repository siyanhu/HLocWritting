% ---------- Algorithms / Pseudocode ----------
\section{Algorithms}\label{sec:algorithms}

% --- Simple algorithm ---
\subsection{Simple Algorithm}

\begin{algorithm}[htbp]
  \caption{Binary Search}\label{alg:binary_search}
  \begin{algorithmic}[1]
    \Require Sorted array $A[1 \ldots n]$, target value $t$
    \Ensure Index $i$ such that $A[i] = t$, or $-1$ if not found
    \State $\ell \gets 1$
    \State $r \gets n$
    \While{$\ell \leq r$}
      \State $m \gets \lfloor (\ell + r) / 2 \rfloor$
      \If{$A[m] = t$}
        \State \Return $m$
      \ElsIf{$A[m] < t$}
        \State $\ell \gets m + 1$
      \Else
        \State $r \gets m - 1$
      \EndIf
    \EndWhile
    \State \Return $-1$
  \end{algorithmic}
\end{algorithm}

% --- Algorithm with functions and loops ---
\subsection{Training Loop}

\begin{algorithm}[htbp]
  \caption{Gradient Descent Training}\label{alg:training}
  \begin{algorithmic}[1]
    \Require Dataset $\mathcal{D}$, learning rate $\eta$, epochs $T$
    \Ensure Trained parameters $\boldsymbol{\theta}^{*}$
    \State Initialise $\boldsymbol{\theta}$ randomly
    \For{$t = 1$ \textbf{to} $T$}
      \For{each mini-batch $\mathcal{B} \subset \mathcal{D}$}
        \State $\mathcal{L} \gets \textsc{ComputeLoss}(\boldsymbol{\theta}, \mathcal{B})$
        \State $\mathbf{g} \gets \nabla_{\boldsymbol{\theta}} \mathcal{L}$
        \State $\boldsymbol{\theta} \gets \boldsymbol{\theta} - \eta \, \mathbf{g}$
      \EndFor
      \If{convergence criterion met}
        \State \textbf{break}
      \EndIf
    \EndFor
    \State $\boldsymbol{\theta}^{*} \gets \boldsymbol{\theta}$
    \State \Return $\boldsymbol{\theta}^{*}$
  \end{algorithmic}
\end{algorithm}

Cross-referencing: \Cref{alg:binary_search} runs in $O(\log n)$ time;
\Cref{alg:training} outlines the standard training procedure.
